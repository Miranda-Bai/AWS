# What Is AWS
On-premises and cloud computing
# AWS Region considerations
1. Compliance 灵活性
2. Latency 延迟
3. Pricing
4. Service availability
# Interacting with AWS
AWS products or services

# What’s the big deal about auth?
authentication: When you create your AWS account, you use the combination of an email address and a password to verify your identity. If a user types in the correct email and password, the system assumes the user is allowed to enter and grants them access. This is the process of authentication.
authorization: Once you’re authenticated and in your AWS account, you might be curious about what actions you can take. This is where authorization comes in. Authorization is the process of giving users permission to access AWS resources and services. Authorization determines whether a user can perform certain actions, such as read, edit, delete, or create resources. Authorization answers the question, “What actions can you perform?” 

## Multi-factor authentication (MFA)
MFA requires two or more authentication methods to verify an identity. MFA pulls from the following three categories of information:

1. Something you know, such as a user name and password, or pin number
2. Something you have, such as a one-time passcode from a hardware device or mobile app
3. Something you are, such as fingerprint or face scanning technology

Using a combination of this information enables systems to provide a layered approach to account access. So even if the first method of authentication, like Bob’s password, is cracked by a malicious actor, the second method of authentication, such as a fingerprint, provides another level of security. This extra layer of security can help protect your most important accounts, which is why you should enable MFA on your AWS root user.
## MFA on AWS
If you enable MFA on your root user, you must present a piece of identifying information from both the something you know category and the something you have category. The first piece of identifying information the user enters is an email and password combination. The second piece of information is a temporary numeric code provided by an MFA device.

Enabling MFA adds an additional layer of security because it requires users to use a supported MFA mechanism in addition to their regular sign-in credentials. Enabling MFA on the AWS root user account is an AWS best practice.
# IAM
AWS Identity and Access Management (IAM) is an AWS service that helps you manage access to your AWS account and resources. It also provides a centralized view of who and what are allowed inside your AWS account (authentication), and who and what have permissions to use and work with your AWS resources (authorization).
## IAM user

An IAM user represents a person or service that interacts with AWS. You define the user in your AWS account. Any activity done by that user is billed to your account. Once you create a user, that user can sign in to gain access to the AWS resources inside your account.

You can also add more users to your account as needed. For example, for your cat photo application, you could create individual users in your AWS account that correspond to the people who are working on your application. Each person should have their own login credentials. Providing users with their own login credentials prevents sharing of credentials.

## Consider the following examples:

A new developer joins your AWS account to help with your application. You create a new user and add them to the developer group, without thinking about which permissions they need.
A developer changes jobs and becomes a security engineer. Instead of editing the user’s permissions directly, you remove them from the old group and add them to the new group that already has the correct level of access.
## Keep in mind the following features of groups:

Groups can have many users.
Users can belong to many groups.
Groups cannot belong to groups.
```{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "cloudwatch:DescribeAlarms*",
                "ec2:Describe*",
                "ec2:StartInstances",
                "ec2:StopInstances",
            ],
            "Resource": "*"
        }
    ]
}
```
# EC2 = Elastic Compute Cloud = Infrastructure as a Service
## Create Instance
> User data script:
```
#!/bin/bash
#Use this for your user data (script from top to bottom)
#install httpd (Linux 2 version)
yum update -y
yum install -y httpd
systemctl start httpd
systemctl enable httpd
echo "<h1>Hello World from $(hostname -f)<h1>/var/www/html/index.html
```
只选SSH和http.
1. It mainly consists in the capability of:
2. Renting vitual machines(EC2)
3. Storing data on virtual drives(EBS)
4. Scaling the services using an auto-scaling group(ASG)

> public IP port may change when you restart the instance

## EC2 User Data
It is possible to bootstrap our instances using an EC2 User Data Script.
Bootstrapping means launching commands when a machine starts.
The script is only run once at the instance first start
#### EC2 user data is used to automate boot task such as:
* Installing updates
* Installing software
* Downloading common files from the Internet
* Anything you can think of.
> The EC2 User Data Script runs with the root user.
Keypair use for SSH
## EC2 Instance Types
* m5.2xlarge
* m:instance class
* 5:generation(AWS improves them over time)
* 2xlarge:size within the instance class
Balance between:
* Compute
* Memory
* Networking
EC2 instance comparison
https://instances.vantage.sh/
## Classic Ports to know
* 22 = SSH(Secure Shell) - log into a Linux instance
* 21 = FTP(File Transfer Protocol) - upload files into a file share
* 22 = SFTP(Secure File Transfer Protocol) - upload files using SSH
* 80 = HTTP - access unsecured websites
* 443 = HTTPS - access secured websites
* 3389 = RDP(Remote Desktop Protocol) - log into a Windows instance
# SSH
|               | SSH | Putty | EC2 instance connect |
|:-------------:|:---:|:-----:|:--------------------:|
| Mac           | Y   |       | Y                    |
| Linux         | Y   |       | Y                    |
| Windows >= 10 | Y   | Y     | Y                    |
| Windows < 10  |     | Y     | Y                    |

## How to SSH into your EC2 Instance using Linux / Mac OS X
![Screenshot 2023-02-06 at 12.09.20 PM](assets/Screenshot%202023-02-06%20at%2012.09.20%20PM.png)
![Screenshot 2023-02-06 at 12.09.36 PM](assets/Screenshot%202023-02-06%20at%2012.09.36%20PM.png)
# EC2 Instances Purchasing Options
1. On-Demand Instances - short workload, predictable pricing, pay by second
2. Reserved(1&3 years)
*     Reserved Instances - long workloads
*     Convertible Reserved Instances - long workloads with flexible instan
3. Saving Plans(1&3 years) -commitment to an amount of usage, long workload
4. Spot Instances - short workloads, cheap, can lose instances(less reliable)
5. Dedicated Hosts - book an entire physical server, control instance placement
6. Dedicated Instances - no other customers will share your hardware
7. Capacity Reservations - reserve capacity in a specific AZ for any duration
# Elastic IP
* With an Elastic IP address, you can mask the failure of an instance or software by rapidly remapping the address to another instance in your account.
* You can only have 5 Elastic IP in your account(you can ask AWS to increase that)
* Overall, try to avoid using Elastic IP, they often reflect poor architectural decisions. Instead, use a random public IP and register a DNS name to it. Or, use a Load Balancer and don't use a public IP.
# ENI Elastic Network Interfaces
Logical component in a VPC that represents a virtual network card
The ENI can have the following attributes:
    Primary private IPv4, one or more secondary IPv4
    One Elastic IP(IPv4) per private IPv4
    One Public IPv4
    One or more security groups
    A MAC address
You can create ENI independently and attach them on the fly (move them) on EC2 instances for failover.
Bound to a specific availability zone (AZ)
# EC2 Hibernate
1. Introducing EC2 Hibernate:
    The in-memory(RAM) state is preserved
    The instance boot is much faster!(the OS is not stopped / restarted)
    Under the hood: the RAM state is written to a file in the root EBS volume
    The root EBS volume must be encrypted
2. Use cases:
    Long-running processing
    Saving the RAM state
    Services that take time to initialize
3. Good to know
    Supported Instance Families - C3,C4,C5,I3,M3,M4,R3,R4,T2,T3....
    Instance RAM Size - must be less than 150GB
    Instance Size - not supported for bare metal instances.
    AMI - Amazon Linux 2, Linux AMI, Ubuntu, RHEL, CentOS & Windows....
    Root Volume - must be EBS, encrypted, not instance store, and large.
    Available for On-Demand, Reserved and Spot Instances
    An instance can NOT be hibernated more than 60 days.
# EC2 Instance Storage Section
## EBS Volume
An EBS (Elastic Block Store) Volume is a network drive you can attach to your instances while they run
It allows your instances to persist data, even after their termination.
They can only be mounted to one instance at a time (at the CCP level)
> Note: CCP - Certified Cloud Practitioner - one EBS can be only mounted to one EC2 instance
> Associate Level (Solutions Architect, Developer, SysOps): "multi-attach" feature for some EBS
They are bound to a specific availability zone
Analogy: Think of them as a "network USB stick"
# Solution Architecture
# S3 Encryption
## SSE-S3
encryption using keys handled & managed by Amazon S3
Object is encrypted server side
AES-256 encryption type
Must set header: "x-amz-server-side-encryption":"AES256"
## SSE-KMS
encryption using keys handled & managed by KMS
KMS advantages: user control + audit trail
Object is encrypted server side
Must set header: "x-amz-server-side-encryption":"aws:kms"
## SSE-C
server-side encryption using data keys fully managed by the customer outside of AWS
Amazon S3 does not store the encryption key you provide
HTTPS must be used
Encryption key must provided in HTTP headers, for every HTTP request made
## Client Side Encryption
Client library such as the Amazon S3 Encryption Client
Clients must encrypt data themselves before sending to S3
Clients must decrypt data themselves when retrieving from S3
Customer fully manages the keys and encryption cycle
# Encryption in transit (SSL/TLS)
Amazon S3 exposes:
    HTTP endpoint: non encrypted
    HTTPS endpoint: encryption in flight
You're free to use the endpoint you want, but HTTPS is recommended
Most clients would use the HTTPS endpoints by default
HTTPS is mandatory for SSE-C
Encryption in flight is also called SSL/TLS

Note: an IAM principal can access an S3 object is
    the user IAM permissions allow it OR the resource policy ALLOWS it.
    AND there's no explicit DENY. 
# CORS
An origin is a scheme (protocol), host(domain) and port
    implied port is 443 for HTTPS, 80 from HTTP
CORS means Cross-Origin Resource Sharing
Web Browser based mechanism to allow requests to other origins while visiting the main origin 

IAM Policy Simulator
https://policysim.aws.amazon.com/home/index.jsp?#roles/MyFirstEC2Role
# AWS EC2 Instance Metadata
It is powerful but one of the least known features to developers
It allows AWS EC2 instance to "learn about themselves" without using an IAM Role for that purpose
The URL is http://169.254.169.254/latest/meta-data
You can retrieve the IAM Role name from the metadata, but you CANNOT retrieve the IAM Policy.
Metadata = Info about the EC2 instance
Userdata = launch script of the EC2 instance
# AWS SDK
We have to use the AWS SDK when coding against AWS Services such as DynamoDB
# Advanced S3
## S3 MFA-Delete
MFA (multi factor authentication) forces user to generate a code on a device (usually a mobile phone or hardware) before doing important operations on S3.
To use MFA-Delete, enable Versioning on the S3 bucket.
You will need MFA to
    permanently delete an object version
    suspend versioning on the bucket
You won't need MFA for
    enabling versioning
    listing deleted versions
Only the bucket owner (root account) can enable/disable MFA-Delete
MFA-Delete currently can only be enabled using the CLI
## S3 Access Logs: Warning
Do not set your logging bucket to be the monitored bucket
It will create a logging loop, and your bucket will grow in size exponentially
## S3 Byte-Range Fetches
Parallelize GETs by requesting specific byte ranges
Better resilience in case of failure
Can be used to speed up downloads
Can be used to retrieve only partial data (for example the head of a file)
# CloudFront
Content Delivery Network (CDN)
Improves read performance, content is cached at the edge
216 Point of Presence globally (edge locations)
DDoS protection, integration with Shielf, AWS Web Application Firewall
Can expose external HTTPS and can talk to internal HTTPS backends
## AWS Global accelerator
Unicast IP: one server holds one IP address
Anycast IP: all servers hold the same IP address and the client is routed to the nearest one
# AWS Snow Family
Highly-secure, portable devices to collect and process data at the edge, and migrate data into and out of AWS
Data migration: Snowcone, Snowball Edge, Snowmobile
Edge computing: Snowcone, Snowball Edge
## Sloution Architecture: Snowball into Glacier
Snowball cannot import to Glacier directly
You must use Amazon S3 first, in combination with an S3 lifecycle policy
# Amazon FSx
Launch 3rd party high-performance file systems on AWS
Fully managed service
FSx for Lustre, FSx for NetApp ONTAP, FSx for Windows File Server, FSx for OpenZFS
## Hybrid Cloud for Storage
AWS is pushing for "hybrid cloud"
    Part of your infrastructure is on the cloud
    Part of your infrastructure is on-premises
This can be due to
    Long cloud migrations
    Security requirements
    Compliance requirements
    IT strategy
S3 is a proprietary storage technology (unlike EFS / NFS), so how do you expose the S3 data on-premises?
    AWS storage gateway
## AWS Storage Cloud Native Options
Block: Amazon EBS, EC2 Instance Store
File: Amazon EFS, Amazon FSx
Object: Amazon S3, Amazon Glacier
## AWS Storage Gateway
Bridge between on-premises data and cloud data
Use cases:
    disaster recovery
    backup & restore
    tiered storage
    on-premises cache & low-latency files access
Types of Storage Gateway:
    S3 File Gateway
    FSx File Gateway
    Volume Gateway
    Tape Gateway
## AWS Transfer Family
A fully-managed service for file transfers into and out of Amazon S3 or Amazon EFS using the FTP protocol
Support Protocols
    AWS Transfer for FTP (File Transfer Protocol (FTP))
    AWS Transfer for FTPS (File Transfer Protocol over SSL (FTPS))
    AWS Transfer for SFTP (Secure File Transfer Protocol (SFTP))
Managed infrastructure, Scalable, Reliable, Highly Available (multi-AZ)
Pay per provisioned endpoint per hour + data transfers in GB
Store and manage users' credentials within the service

## AWS DataSync
Move large amount of data to and from
    on-premises/other cloud to AWS (NFS, SMB, HDFS, S3 API...)- needs agent
    AWS to AWS (different storage services) - no agent needed
Can synchronize to:
    Amazon S3 (any storage classes - including Glacier)
    Amazon EFS
    Amazon FSx (Windows, Lustre, NetApp, OpenZFS...)
Replication tasks can ebe scheduled hourly, daily, weekly
File permissions and metadata are preserved (NFS POSIX, SMB...)
One agent task can use 10Gbps, can setup a bandwidth limit
# AWS Integration and Messaging
Synchronous between applications can be problematic if there are sudden spikes of traffic
What if you need to suddenly encode 1000 videos but usually it's 10?
In that case, it's better to decouple your applications
    using SQS: queue model
    using SNS: pub/sub model
    using Kinesis: real-time streaming model
These Services can scale independently from our application
## Amazon SQS -  Standard Queue
Oldest offering (over 10 years old)
Fully managed service, used to decouple applications
Attributes:
    Unlimited throughput, unlimited number of messages in queue
    Default retention of messages: 4 days, maximum of 14 days
    Low latency (<10 ms on publish and receive)
    Limitation of 256KB per message sent
Can have duplicate messages (at least once delivery, occasionally)
Can have out of order messages (best effort ordering)
## SQS - Producing Messages
Produced to SQS using the SDK (SendMessage API)
The message is persisted in SQS until a consumer deletes it
Message retention: default 4 days, up to 14 days
Example: send an order to be processed
    Order id
    Customer id
    Any attributes you want
SQS standard: unlimited throughput(吞吐量)
## SQS - Consuming Messages
Consumers (running on EC2 instances, servers, or AWS Lambda)...
Poll(测验,调查) SQS for messages (receive up to 10 messages at a time)
## SQS - Multiple EC2 Instances Consumers
Consumers receive and process messages in parallel
At least once delivery
Best-effort message ordering
Consumers delete messages after processing them
We can scale consumers horizontally to improve throughput of processing
## Amazon SQS - Security
Encryption
    In-flight encryption using HTTPS API
    At-rest encryption using KMS keys
    Client-side encryption if the client wants to perform encryption/decryption iteself
Access Controls: IAM policies to regulate access to the SQS API
SQS Access Policies (similar to S3 bucket policies)
    Useful for cross-account access to SQS queues
    Useful for allowing other services (SNS,S3...) to write to an SQS queue
## SQS - Message Visibility Timeout
After a message is polled by a consumer, it becomes invisible to other consumers
By default, the "messge visibility timeout" is 30 seconds
That means the message has 30 seconds to be processed
After the message visibility timeout is over, the message is "visible" in SQS
## Amazon SQS - Long Polling
When a consumer requests messages from the queue, it can optionally "wait" for messages to arrive if there are none in the queue
This is called Long Polling
LongPolling decreases the number of API calls made to SQS while increasing the efficiency and latency of your application
The wait time can be between 1s to 20s (20s preferable)
Long Polling is preferable to Short Polling
Long polling can be enabled at the queue level or at the API level using WaitTimeSeconds.
## Amazon SQS - FIFO Queue
FIFO = First In First Out (ordering of messages in the queue)
Limited throughput: 300 msg/s without batching, 3000 msg/s with
Exactly-once send capability (by removing duplicates)
Messages are processed in order by the consumer
## SQS with Auto Scaling Group (ASG)
If the load is too big, some transactions may be lost
SQS as a buffer to database writes
## Amazon SNS - Simple Notification Service
The "event producer" only sends message to one SNS topic
As many "event receivers" (subscriptions) as we want to listen to the SNS topic notifications
Each subscriber to the topic will get all the message (note:new feature to filter messages)
Up to 12,500,000 subscriptions per topic
100,000 topics limit
## SNS integrates with a lot of AWS services
Many AWS services can send data directly to SNS for notification
## AWS SNS - How to publish
Topic Publish (using the SDK)
    Create a topic
    Create a subscription (or many)
    Publish to the topic
Direct Publish (for mobile apps SDK)
    Create a platform application
    Create a platform endpoint
    Publish to the platform endpoint
    Works with Google GCM, Apple APNS, Amazon ADM...
## Amazon SNS - Security
Encryption
    In-flight encryption using HTTPS API
    At-rest encryption using KMS keys
    Client-side encryption if the client wants to perform encryption/decryption iteself
    
Access Controls: IAM policies to regulate access to the SNS API

SQS Access Policies (similar to S3 bucket policies)
    Useful for cross-account access to SNS topics
    Useful for allowing other services (S3...) to write to an SNS topic
## SNS + SQS: Fan Out
Push once in SNS, receive in all SQS queues that are subscribers
Fully decoupled, no data loss
SQS allows for: data persistence, delayed processing and retries of work
Ability to add more SQS subscribers over time
Make suer your SQS queue access policy allows for SNS to write
## Application: S3 Events to multiple queues
For the same combination of: event type (e.g. object create) and prefix(e.g.images/) you can only have one S3 Event rule
If you want to send the same S3 event to many SQS queues, use fan-out
## Application: SNS to Amazon S3 through Kinesis Data Firehose
SNS can send to Kinesis and thereforee we can have the following solutions architecture:
Buying Service --> SNS Topic --> Kinesis Data Firehose --> Amazon S3
## Amazon SNS - FIFO Topic
Similar features as SQS FIFO
Can only have SQS FIFO queues as subscribers
Limited throughput (same throughput as SQS FIFO)
In case you need fan out + ordering + deduplication(重复数据删除)
## SNS - Message Filtering
JSON policy used to filter messages send to SNS topic's subscriptions
If a subscription doesn't have a filter policy, it receives every message

# Kinesis
Makes it easy to collect, process, and analyze streaming data in real-time
Ingest real-time data such as: Application logs, Metrics, Website clickstreams, IoT telemetry data...
Kinesis Data Streams: capture, process, and store data streams
Kinesis Data Firehose: load data streams into AWS data stores
Kinesis Data Analytics: analyze data streams with SQL or Apahe Flink
Kinesis Video Streams: capture, process, and store video streams
## Kinesis Data Streams
Retention(保留) between 1 day to 365 days
Ability to reprocess (replay) data
Once data is inserted in Kinesis, it can't be deleted (immutability)
Data that shares the same partition goes to the same shard (ordering)
Producers: AWS SDK, Kinesis Producer Library(KPL), Kinesis Agent
Consumers:
    Write your own: Kinesis Client Library(KCL), AWS SDK
    Managed: AWS Lambda, Kinesis Data Firehose, Kinesis Data Analytics
## Kinesis Data Streams - Capacity Modes
Provisioned mode:
    You choose the number of shards provisioned, scale manually or using API
    Each shard gets 1 MB/s in (or 1000 records per second)
    Each shard gets 2 MB/s out (classic or enhanced fan-out consumer)
    You pay per shard provisioned per hour
On-demand mode:
    No need to provision or manage the capacity
    Default capacity provisioned (4 MB/s in or 4000 records per second)
    Scales automatically based on observed throughput peak during the last 30 days
    Pay per stream per hour & data in/out per GB
## Kinesis Data Streams Security
Control access / authorization using IAM policies
Encryption in flight using HTTPS endpoints
Encryption at rest using KMS
You can implement encryption/decryption of data on client side(harder)
VPC Endpoints available for Kinesis to access within VPC
Monitor API calls using CloudTrail(云轨)
# Container
## Docker
Docker is a software development platform to deploy apps
Apps are packaged in containers that can be run on any OS
Apps run the same, regardless of where they're run
    Any machine
    No compatibility issues
    Predictable behavior
    Less work
    Easier to maintain and deploy
    Works with any language, any OS, any technology
Use cases: microservices architecture, lift-and-shift apps from on-premises to the AWS cloud...
## Docker versus Virtual Machines
Docker is "sort of" a virtualization technology, but not exactly
Resources are shared with the host => many containers on one server
## Docker Containers Management on AWS
Amazon Elastic Container Service (Amazon ECS)
    Amazon's own container platform
Amazon Elastic Kubernetes Service (Amazon EKS)
    Amazon's managed Kubernetes (open source)
## Amazon ECS - EC2 Launch Type
ECS = Elastic Container Service
Launch Docker containers on AWS = Launch ECS Tasks on ECS Clusters
EC2 Launch Type: you must provision & maintain the infrastructure (the EC2 instances)
Each EC2 Instance must run the ECS Agent to register in the ECS Cluster
AWS takes care of starting/stopping containers
## Amazon ECS - Fargate Launch Type
Launch Docker containers on AWS
You do not provision the infrastructure (no EC2 instances to manage)
It's all Serverless
You just create task definitions
AWS just runs ECS Tasks for you based on the CPU / RAM you need
To scale, just increase the number of tasks. Simple - no more EC2 instances
Amazon ECS - IAM Roles for ECS
EC2 instance Profile (EC2 Launch Type only):
    Used by the ECS angent
    Makes API calls to ECS service
    Send container logs to CloudWatch Logs
    Pull Docker image from ECR
    Reference sensitive data in Secret Manager or SSM Parameter Store
ECS Task Role:
    Allows each task to have a specific role
    Use different roles for the different ECS Services you run
    Task Role is defined in the task definition
## Amazon ECS - Load Balancer Integrations
* Application Load Balancer supported and works for most use cases
* Network Load Balancer recommended only for high throughput / high performance use cases, or to pair it with AWS Private Link
* Elastic Load Balancer supported but not recommended (no advanced features - no Fargate)
## Amazon ECS -  Data Volumes (EFS)
Mount EFS file systems onto ECS tasks
Works for both EC2 and Fargate launch types
Tasks running in any AZ will share the same data in the EFS file system
Fargate + EFS = Serverless
Use cases: persistent multi-AZ shared storage for your containers
Note:
    Amazon S3 cannot be mounted as a file system
## ECS Service Auto Scaling
Automatically increase/decrease the desired number of ECS tasks
Amazon ECS Auto Scaling uses AWS Application Auto Scaling
    ECS Service Average CPU Utilization
    ECS Service Average Memory Utilization - Scale on RAM
    ALB Request Count Per Target - metric coming from the ALB
Target Tracking - scale based on target value for a specific CloudWatch metric
Step Scaling - scale based on a specified CloudWatch Alarm
Scheduled Scaling - scale based on a specified date/time (predictable changes)

ECS Service Auto Scaling (task level) ≠ EC2 Auto Scaling (EC2 instance level)
Fargate Auto Scaling is much easier to setup (because Serverless)
## EC2 Launch Type - Auto Scaling EC2 Instances
Accommodate ECS Service Scaling by adding underlying EC2 Instances

Auto Scaling Group Scaling
    Scale your ASG based on CPU Utilization
    Add EC2 instances over time

ECS Cluster Capacity Provider
    Used to automatically provision and scale the infrastructure for your ECS Tasks
    Capacity Provider paired with an Auto Scaling Group
    Add EC2 Instances when you're missing capacity (CPU, RAM...)

# Serverless
Initially... Serverless == Faas(Function as a Service)
Serverless was pioneered by AWS Lambda but now also includes anything that's managed: "database, messaging, storage, etc."

Serverless does not mean there are no servers...
it means you just don't manage / provision / see them
# Serverless in AWS
AWS Lambda
DynamoDB
AWS Cognito
AWS API Gateway
Amazon S3
AWS SNS & SQS
AWS Kinesis Data Firehose
Aurora Serverless
Step Functions
Fargate
## AWS Lambda
Virtual functions - no servers to manage
Limited by time - short executions
Run on-demand(按需)
Scaling is automated
## Benefits of AWS Lambda
Easy Pricing:
    Pay per request and compute time
    Free tier of 1,000,000 AWS Lambda requests and 400,000 GBs of compute time
Integrated with the whole AWS suite of services
Integrated with many programming languages
Easy monitoring through AWS CloudWatch
Easy to get more resources per functions (up to 10GB of RAM)
Increasing RAM will also improve CPU and network
## AWS Lambda language support
Node.js (Javascript)
Python
Java (Java 8 compatible)
C# (.Net Core)
Golang
C# / Powershell
Ruby
Custom Runtime API (community supported, example Rust)

Lambda Container Image
    The container image must implement the Lambda Runtime API
    ECS / Fargate is preferred for running arbitrary Docker images
## AWS Lambda Integrations
Main ones
API Gateway, Kinesis, DynamoDB, S3, CloudFront, CloudWatch Events EventBridge, CloudWatch Logs, SNS, SQS, Cognito
## AWS Lambda Limits to Know - per region
Execution:
    Memory allocation: 128MB - 10GB (1MB increments)
    Maximum execution time: 900 seconds (15 minutes)
    Environment variable (4KB)
    Disk capacity in the "function container"(in/tmp):512MB to 10GB
    Concurrency executions:1000 (can be increased)
Deployment:
    Lambda function deployment size (compressed.zip): 50MB
    Size of uncompressed deployment (code + dependencies): 250MB
    Can use the /tmp directory to load other files at startup
    Size of environment variable: 4KB
## Lambda by default
By default, your Lambda function is launched outside your own VPC (in an AWS-owned VPC)
Therefore, it cannot access resources in your VPC (RDS, ElasticCache, internal ELB...)

**Lambda in VPC**
You must define the VPC ID, the Subnets and the Security Groups
Lambda will create an ENI (Elastic Network Interface) in your subnets

**Lambda with RDS Proxy**
If Lambda functions directly access your database, they may open too many connections under high load

**RDS Proxy**
    Improve scalability by pooling and sharing DB connections
    Improve availability by reducing by 66% the failover time and preserving connections
    Improve security by enforcing IAM authentication and storing credentials in Secrets Manager
> The Lambda function must be deployed in your VPC, because RDS Proxy is never publicly accessible

## Amazon DynamoDB
Fully managed, highly available with replication across multiple AZs
NoSQL database - not a relational database - with transaction support
Scales to massive workloads, distributed database
Millions of requests per second, trillions of row, 100s of TB of storage
Fast and consistent in performance (single-digit millisecond)
Integrated with IAM for security, authorization and administration
Low cost and auto-scaling capabilities
No maintenance(维护) or patching, always available
Standard & Infrequent Access (IA) Table Class
## DynamoDB - Basics
DynamoDB is made of Tables
Each table has a Primary Key (must be decided at creation time)
Each table can have an infinite number of items (=rows)
Each items has attributes (can be added over time - can be null)
Maximum size of an item is 400KB
Data types supported are:
    Scalar Types - String, Number, Binary, Boolean, Null
    Document Types - List, Map
    Set Types - String Set, Number Set, Binary Set
> Therefore, in DynamoDB you can rapidly evolve schemas
# Database
## Choosing the Right Database
We have a lot of managed databases on AWS to choose from
Questions to choose the right database based on your architecture:
    Read-heavy, write-heavy, or balanced workload? Throughput needs? Will it change, does it need to scale or fluctuate during the day?
    How much data to store and for how long? Will it grow? Average object size? How are they accessed?
    Data durability? Source of truth for the data?
    Latency requirements? Concurrent users?
    Data model? How will you query the data? Joins? Structured? Semi-Structured?
    Strong schema? More flexibility? Reporting? Search? RDBMS / NoSQL?
    License costs? Switch to Cloud Native DB such as Aurora?
## Database Types
* RDBMS (=SQL/OLTP): RDS, Aurora - great for joins
* NoSQL database - no joins, no SQL: DynamoDB(~JSON), ElastCache(key/value pairs), Neptune(graphs), DocumentDB(for MongoDB), Keyspaces(for Apache Cassandra)
* Object Store: S3(for big objects) / Glacier(for backups / archives)
* Data Warehouse (= SQL Analytics / BI): Redshift(OLAP), Athena, EMR
* Search: OpenSearch (JSON) - free text, unstructured searches
* Graphs: Amazon Neptune - displays relationships between data
* Ledger: Amazon Quantum Ledger Database
* Time series: Amazon Timestream
> Note: some databases are being discussed in the Data & Analytics section
## Amazon RDS - Summary
Managed PostgreSQL / MySQL / Oracle / SQL Server / MariaDB / Custom
Provisioned RDS Instance Size and EBS Volume Type & Size
Auto-scaling capability for Storage
Support for Read Replicas and Multi AZ
Security through IAM, Security Groups, KMS, SSL in transit
Automated Backup with Point in time restore feature (up to 35 days)
Manual DB Snapshot for longer-term recovery
Managed and Scheduled maintenance (with downtime)
Support for IAM Authentication, integration with Secrets Manager
RDS Custom for access to and customize the underlying instance (Oracle & SQL Server)
**Use case:** Store relational datasets (RDBMS / OLTP), perform SQL queries, transactions
## Amazon Aurora - Summary
Compatible API for PostgreSQL / MySQL, separation of storage and compute
Storage: data is stored in 6 replicas, across 3 AZ - highly available, self-healing, auto-scaling
Compute: Cluster of DB Instance across multiple AZ, auto-scaling of Read Replicas
Cluster: Custom endpoints for writer and reader DB instances
Same security / monitoring / maintenance features as RDS
Know the backup & restore options for Aurora
**Aurora Serverless** - for unpredictable / intermittent workloads, no capacity planning
**Aurora Multi-Master** - for continuous writes failover (high write availability)
**Aurora Global**: up to 16 GB Read Instances in each region, < 1 second storage replication
**Aurora Machine Learning:** perform ML using SageMaker & Comprehend on Aurora
**Aurora Database Cloning:** new cluster from existing one, faster then restoring a snapshot
**Use case:** same as RDS, but with less maintenance / more flexibility / more performance / more features
## Amazon ElastiCache - Summary
Managed Redis / Memcached (similar offering as RDS, but for caches)
In-memory data store, sub-millisecond latency
Must provision an EC2 instance type
Support for Clustering (Redis) and Multi AZ, Read Replicas (sharding)
Security through IAM, Security Groups, KMS, Redis Auth
Backup / Snapshot / Point in time restore feature
Managed and Scheduled maintenance
**Requires some application code changes to be leveraged**
**Use Case:** Key/Value store, Frequent reads, less writes, cache results for DB queries, store session data for websites, cannot use SQL
## Amazon DynamoDB - Summary
AWS proprietary technology, managed serverless NoSQL database, millisecond latency
Capacity modes: provisioned capacity with optional auto-scaling or on-demand capacity
Can replace ElatiCache as a key/value store (storing session data for example, using TTL feature)
Highly Available, Multi AZ by default, Read and Writes are decoupled, transation capability
DAX cluster for read cache, microsecond read latency
Security, authentication and authorization is done through IAM
Event Processing: DynamoDB Streams to integrate with AWS Lambda, or Kinesis Data Streams
Global Table feature: active-active setup
Automated backups up to 35 days with PITR (restore to new table), or on-demand backups
Export to S3 without using RCU within the PITR window, import from S3 without using WCU
**Great to rapidly evolve schemas**
**Use Case:** Serverless applications development (small documents 100s KB), distributed serverless cache, doesn't have SQL query language available
## Amazon S3 - Summary
S3 is a ... key/value store for objects
Great for bigger objects, not so great for many small objects
Serverless, scales infinitely, max object size is 5 TB, versioning capability
Tiers: S3 Standard, S3 Infrequent Access, S3 Intelligent, S3 Glacier + lifecycle policy
Features: Versioning, Encryption, Replication, MFA-Delete, Access Logs...
Security: IAM, Bucket Policies, ACL, Access Points, Object Lambda, CORS, Object/Vault Lock
Encryption: SSE-S3, SSE-KMS, SSE-C, client-side, TLS in transit, defualt encryption
Batch operations on objects using S3 Batch, listing files using S3 Inventory
Performance: Multi-part upload, S3 Transfer Acceleration, S3 Select
Automation: S3 Event Notifications (SNS, SQS, Lambda, EventBridge)

Use Cases: static files, key value store for big files, website hosting
## DocumentDB
Aurora is an "AWS-implementation" of PostgreSQL / MySQL
DocumentDB is the same for MongoDB (which is a NoSQL database)

MongoDB is used to store, query, and index JSON data
Similar "deployment concepts" as Aurora
Fully managed, highly available with replication across 3 AZ
Aurora storage automatically grows in increments of 10GB, up to 64TB

Automatically scales to workloads with millions of requests per seconds
## Amazon Neptune
Fully managed graph database
A popular graph dataset would be a social network
    Users have friends
    Posts have comments
    Comments have likes from users
    Users share and like posts...
Highly available across 3 AZ, with up to 15 read replicas
Build and run applications working with highly connected datasets - optimized for these complex and hard queries
Can store up to billions of relations and query the graph with milliseconds latency
Highly avaiable with replications across multiple AZs
Great for knowledge graphs (Wikipedia), fraud detection, recommendation engines, social networking
## Amazon Keyspaces (for Apache Cassandra)
Apache Cassandra is an open-source NoSQL distributed database
A managed Apache Cassandra - compatible database service
Serverless, Scalable, highly available, fully managed by AWS
Automatically scale talbes up/down based on the application's traffic
Tables are replicated 3 times across multiple AZ
Using the Cassandra Query Language (CQL)
Single-digit millisecond latency at any scale, 1000s of requests per second
Capacity: On-demand mode or provisioned mode with auto-scaling
Encryption, backup, Point-In-Time Recovery (PITR) up to 35 days

Use cases: store IoT devices info, time-series data,...
## Amazon QLDB
QLDB stands for "Quantum Ledger Database"
A ledger is a book recording financial transactions
Fully managed, Serverless, High available, Replication across 3 AZ
Used to **review history of all the changes made to your application data** over time
Immutable system: no entry can be removed or modified, cryptographically verifiable
2-3x better performance than common ledger blockchain frameworks, manipulate data using SQL
Difference with Amazon Managed Blockchain: **no decentralization component**, in accordance with financial regulation rules
## Amazon Timestream
Fully managed, fast, scalable, serverless time series database
Automatically scales up/down to adjust capacity
Store and analyze trillions of events per day
1000s times faster & 1/10th the cost of relational database
Scheduled queries, multi-measure records, SQL compatibility
Data storage tiering: recent data kept in memory and historical data kept in a cost-optimized storage
Build-in time series analytics functions (helps you identify patterns in your data in near real-time)
Encryption in transit and at rest

Use cases: IoT apps, operational applications, real-time analytics,...
# Amazon Athena
Serverless query service to analyze data stored in Amazon S3
Uses standard SQL language to query the files (built on Presto)
Supports CSV, JSON, ORC, Avro, and Parquet
Pricing: $5.00 per TB of data scanned
Commonly used with Amazon Quicksight for reporting/dashboards
**Use cases**: Business intelligence / analytics / reporting, analyze & query VPC Flow Logs, ELB Logs, **CloudTrail trails**, etc....

## Amazon Athena - Performance Improvement
Use columnar data for cost-savings (less scan)
    Apache Parquet or ORC is recommended
    Huge performance improvement
    Use Glue to convert your data your Parquet or ORC
Compress data for smaller retrievals (bzip2, gzip, lz4, snappy, zlip, zstd...)
Partition datasets in S3 for easy querying on virtual columns
Use larger files (> 128MB) to minimize overhead
# Redshift Overview
Redshift is based on PostgreSQL, but it's not used for OLTP
It's OLAP - online anlytical processing (Analytics and data warehousing)
10x better performance than other data warehouses, scale to PBs of data
Columnar storage of data (instead of row based) & parallel query engine
Pay as you go based on the instances provisoned
Has a SQL interface for performing the queries
Bl tools such as Amazon Quicksight or Tableau integrate with it
**vs Athena**: faster queries / joins / aggregations thanks to indexes
## Redshift Cluster
Leader node: for query planning, results aggregation
Compute node: for performing the queries, send results to leader
You provison the node size in advance
You can used Reserved Instances for cost savings
## Redshift - Snapshots & DR
Redshift has no "Multi-AZ" mode
Snapshots are point-in-time backups of a cluster, stored internally in S3
Snapshots are incremental (only what has changed is saved)
You can restore a snapshot into a new cluster
Automated: every 8 hours, every 5GB, or on a schedule. Set retention
Manual: snapshot is retained until you delete it
You can configure Amazon Redshift to automatically copy snapshots (automated or manual) of a cluster to another AWS Region
## Redshift Spectrum
Query data that is already in S3 without loading it
Must have a Redshift cluster available to start the query
The query is then submitted to thousands of Redshift Spectrum nodes
# Amazon OpenSearch Service
Amazon OpenSearch is successor to Amazon ElasticSearch
In DynamoDB, queries only exist by primary key or indexes...
With OpenSearch, you can search any field, even partially matches
It's common to use OpenSearch as a complement to another database
OpenSearch requires a cluster of instances (not serverless)
Does not support SQL (it has its own query language)
Ingestion from Kinesis Data Firehose, AWS IoT, and CloudWatch Logs
Security through Cognito & IAM, KMS encryption, TLS
Comes with OpenSearch Dashboards (visualization)
# Amazon EMR
EMR stands for Elastic MapReduce(分布式计算系统)
EMR helps creating Hadoop clusters (Big Data) to analyze and process vast amount of data
The clusters can be made of hundreds of EC2 instances
EMR comes bundled with Apache Spark, HBase, Presto, Flink...
EMR takes care of all the provisioning and configuration
Auto-scaling and integrated with Spot instances

**Use cases:** data processing, machines learning, web indexing, big data...
## Amazon EMR- Node types & purchasing
Master Node: Manage the cluster, coordinate, manage health - long running
Core Node: Run tasks and store data - long running
Task Node (optional): Just to run tasks - usually Spot
Purchasing options:
    On-demand: reliable, predictable, won't be terminated
    Reserved (min 1 year): cost savings (EMR will automatically use if available)
    Spot Instances: cheaper, can be terminated, less reliable
Can have long-running cluster, or transient (temporary) cluster
# Amazon QuickSight
Serverless machine learning-powered business intelligence service to create interactive dashboards
Fast, automatically scalable, embeddable, with per-session pricing
Use cases:
    Business analytics
    Building visualizations
    Perform ad-hoc analysis
    Get business insights using data
Integrated with RDS, Aurora, Athena, Redshift, S3...
In-memory computation using SPICE engine if data is imported into QuickSight
Enterprise edition: Possibility to setup Column-Level security(CLS)
## QuickSight - Dashboard & Analysis
Define Users (standard version) and Groups (enterprise version)
    These users & groups only exist within QuickSight, not IAM
A dashboard...
    is a read-only snapshot of an analysis that you can share
    preserves the configuration of the analysis (filtering, parameters, controls, sort)

You can share the analysis or the dashboard with Users or Groups
To share a dashboard, you must first publish it
Users who see the dashboard can also see the underlying data
# AWS Glue
Managed extract, transform, and load (ETL) service
Useful to prepare and transform data for analytics
Fully serverless service
# Kinesis Data Analytics for SQL applications
Real-time analytics on Kinesis Data Streams & Firehose using SQL
Add reference data from Amazon S3 to enrich streaming data
Fully managed, no servers to provision
Automatic scaling
Pay for actual consumption rate
Output:
    Kinesis Data Stream: create streams out of the real-time analytics queries
    Kinesis Data Firehose: send analytics query results to destinations
Use cases:
    Time-series analytics
    Real-time dashboards
    Real-time metrics
## Kinesis Data Analytics for Apache Flink
Use Flink (Java, Scala or SQL) to process and analyze streaming data
Run any Apache Flink application on a managed cluster on AWS
    provisioning compute resources, parallel computation, automatic scaling
    application backups (implemented as checkpoints and snapshots)
    Use any Apache Flink programming features
    Flink does not read from Firehose (use Kinesis Analytics for SQL instead)
# Amazon Managed Streaming for Apache Kafka (Amazon MSK)
Alternative to Amazon Kinesis
Fully managed Apache Kafka on AWS
    Allow you to create, update, delete clusters
    MSK creates & manages Kafka brokers nodes & Zookeeper nodes for you
    Deploy the MSK cluster in your VPC, multi-AZ (up to 3 for HA)
    Automatic recovery from common Apache Kafka failures
    Data is stored on EBS volumes for as long as you want
    
MSK Serverless
    Run Apache Kafka on MSK without managing the capacity
    MSK automatically provisions resources and scales compute & storage
## Kinesis Data Streams VS Amazon MSK 
| Kinesis Data Streams      | Amazon MSK                                  |
|:--------------------------|:--------------------------------------------|
| 1MB message size limit    | 1MB default, configure for higher (ex:10MB) |
| Data Streams with Shards  | Kafka Topics with Partitions                |
| Shard Splitting & Merging | Can only add partitions to a topic          |
| TLS In-flight encryption  | PLAINTEXT or TLS In-flight Encryption       |
| KMS at-rest encryption    | KMS at-rest encryption                      |

# Big Data Ingestion Pipeline
We want the ingestion pipeline to be fully serverless
We want to collect data in real time
We want to transform the data
We want to query the transformed data using SQL
The reports created using the queries should be in S3
We want to load that data into a warehouse and create dashboards
## Big Data Ingestion Pipeline
IoT devices -->Kinesis data streams-->kinesis data firehose --> S3....


